### Какие эксперементы проводил:
* Сравнение looped трансформеров с разным количеством слоев (больше -> сходится быстрее, но потом выравниваются и ведут себя идентично)
* SSM умеет в эту задачу в плане in context learning (сослаться на статью https://arxiv.org/pdf/2402.03170)
* Сравнение looped tranformer vs looped SSM 
* SSM с разным количеством блоков
* **TODO:** dynamic dropout mask on the input data on each step
* **TODO:** как возрастание количества in-context семплов влияет на качество transformer/ssm (одинаковая корреляция)

### Формат описания эксперемента
* Описание идеи
* Как имплементил
* Результаты
* С какими проблемами столкнулся (если были)
* Выводы (что значат результаты)

---
# Текст

# Отчет по исследованию Looped Transformers

## 1. Введение
В ходе выполнения задания Я работал со статьей [Looped Transformers are better at learning learning algorithms](https://arxiv.org/abs/2311.12424v3), в рамках которой рассматривался вариант решения задачи in-context learning'а - решение задачи линейной регрессии. 
**Постановка проблемы:**
* Имеется набор входных данных $(x_1, y_1, x_2, y_2, \dots, x_k, y_k, x_{test})$, где $x_i \in \mathbb{R}^n$ - входные данные для задачи линейной регресси размерности n, $y_i \in \mathbb{R}$ - скаляр ответ линейной регрессии. 
* Задача модели предсказать вектор весов коеффициентов линейной регресси $w_i \in \mathbb{R}^n$. (Так как $Xw = y$, то это тоже самое, что предсказывать $y_i$).
* Обджектив в таком сетапе выглядит так - $\min_w \|Xw - y\|_2^2$ . Минимизируем MSE для заданного контекста $\rightarrow$ минимируем ошибку предсказания $y_{test}$, который нам неизвестен. 
**Основная идея работы:**
Один из вариантов решения задачи линейной регрессии - итеративно решать задачу с помощью градиентного спуска,  таким образом постепенно уменьшая ошибку.  
Для адаптации к подобному способу решения задачи, адаптировали архитектуру Looped Transformer. В ней инпут несколько раз проходит через  модель, постепенно уменьшая ошибку вычисления.
![paper main idea](pics/idea.png)
## 2. Методология
**В ходе работы я эксперементировал со следующими частями архитектуры модели и процесса обучения:**
1) В статье описывается метод input injection, который заключается в следующем: Для каждой итерации Looped Transformer на вход дополнительно с состоянием после предыдущей итерации подается дополнительно input вектор.  В ходе работы я исследовал, как варьирование количества информации из инпута будет влиять на схождение модели.
2) Я исследовал, как количество блоков трансформера, которые включает в себя один цикл Looped Transformer будет влиять на скорость схождения обучения. 
3) Я исследовал, как изменится процесс обучения, если трансформерный блок заменить на SSM

 **Сетап обучения, использовавшийся в процессе экспериментов:**
 Для обучения я использовал сетап sheduled training'a, который был описан в статье. В таком сетапе задача постепенно изменяется в процессе обучения, для лучшего схождения. Я использовал следующие параметры:
 ```yaml
 training:
    batch_size: 64
    task_name: linear_regression
    learning_rate: 0.0001
    train_steps: 10000
    save_every_steps: 1000
    keep_every_steps: 100000
    curriculum:
        dims:
            start: 5
            end: 5
            inc: 1
            interval: 5000
        points:
            start: 11
            end: 41
            inc: 2
            interval: 500
        loops:
            start: 20
            end: 350
            inc: 2
            interval: 500
    n_loop_window: 20
```
* Обучение включает в себя 10000 шагов
* Размерность входных данных для регрессии ($n$, где $x \in \mathbb{R}^n$) фиксирована в процессе обучения $n=5$
* Количество in-context примеров для обучения растет в процессе обучения. Изначально модель получает на вход 11 примеров. Каждые 500 итераций количество примеров растет на 2. Макимальное количество примеров, которые модель будет видеть в процесс обучения - 41
* Количество итераций Looped Transformer'a изначально равно 20. Каждые 500 итераций вместе с увеличением количества семплов конекста, количество итераций также увеличивается на 2. Таким образом в конце обучения количество итераций - 58. 
* В процессе обучения в каждого шага градиент вычисляется только для 20 шагов, для того, чтобы ускорить обучение и стабилизировать градиент. 
## 3. Эксперименты и результаты
### 3.1 Information Flow
**Описание эксперимента:**
Information flow. Оказывается, что если наивно делать Universal Transformer, то ничего не получится – нужно еще подавать числа из инпута.  В описании задачи говорится следующее:
`То есть, они конкатенируют инпут так, чтобы модель не забывала значение исходных чисел`
Но в исходном коде статьи происходит не конкатенация векторов, а их сложение(или поэлементное умножение):
```python
def f(self, output, embeds):
	if self.loop_func == "z=f(x+z)":
		f_output = self._backbone(inputs_embeds=output + embeds)  # [B, 2n + 1, d]
	elif self.loop_func == "z=f(x*z)":
		f_output = self._backbone(inputs_embeds=output * embeds)  # [B, 2n + 1, d]
	else:
		raise NotImplementedError
	return f_output
```
Исходя из этого, я решил немного поменять сетап эксперемнта.

**Гипотеза:**
Модели не обязательно иметь весь инпут вектор на каждой итерации, достаточно будет его части.

**Процесс проведения:**
Для того, чтобы проверить, как будет вести себя лосс в сетапе, когда с инпута передается не вся информация, а только ее часть, я решил применить динамичскую маскировку входных данных. 
Маскировка данных строится следующем образом: На каждой итерации цикла Looped Transformer'a мы передаем не весь инпут вектор. Каждый елемент входного вектора зануляется с вероятностью $p$. Таким образом, модель получает только $(1-p)\%$ входных данных на каждой итерации. 
В среднем, за все итерации модель увидит весь входной вектор, но во время одной итерации часть входных данных всегда не подается.

**Результаты:**
На графике представлены результаты обучения модели со следующими параметрами:
```yaml
model:
    family: gpt2_loop
    n_embd: 256
    n_layer: 1
    n_head: 8
    n_dims: 20
    n_positions: 101
    apply_input_mask: True
    p: <настраивается>
```
Здесь $p$ означает вероятность для определенного input токена быть замаскированным на каждой из итераций Looped Transformer'a.
Loss означает усредненное значение MSE для каждого из входных семплов.

Параметр $p$ принимает следующие значения:
* $p=0$ (обычный Looped transformer)
* $p=0.15$
* $p=0.3$
* $p=0.5$
* $p=0.7$
* $p=1$ (Весь инпут вектор зануляется)
![dynamic masking results](pics/dynamic_masking.svg)

**Анализ результатов:**

В результате этого эксперемента видно, что при незначительной маскировке входных данных$(p=0.15, p=0.3)$, метод сходится почти почти идентично сетапу, когда модель получает полный инпут вектор на каждом шаге. И даже с случае, когда на каждой итерации маскируется половина входных данных $(p=0.5)$  спустя некоторое количество шагов метод сходится к значению, идентичному обычному обучению Loooped Transformer.
Примечательно, что при достаточно большом зашумлении данных $(p=0.7)$ метод повел себя идентично сетапу, когда входные данные вообще не подаются в Looped Transformer. Предположительно, что при слишком большом зашумлении модель воспринимает инпут вектор как шум.

Из полученных результатов можно сделать вывод, что input injection является надежным методом, который сохраняет производительность даже при значительной маскировке входных данных. Изначальная гипотеза подтвердилась.

Я также провел эксперементы для преверки предлагаемой гипотезы:
`Модель имеет возможность использовать часть токенов как хранилище информации с предыдущего шага.`

Можно ли использовать меньше токенов для этого? то есть, подавать с предыдущего шага не все токены, а только n последних?

**Процесс проведения:**

Для проверки этой гипотезы я провел эксперименты по маскировке части выходного состояния Looped Transformer'a для каждой из итераций. 

То есть, если изначально выходное состояния имеет размерность $o \in \mathbb{R}^{B \times 2n \times d}$, где $B$ - размер батча, $n$ - количество примеров в контексте ($2n$, так как подается и $x_i$ и $y_i$), $d$ - размерность скрытого состояния модели. Допустим, что мы хотим обнулить $k$ первых токенов, тогда размерность маски будет $M \in \mathbb{R}^{B \times k \times d}$. 

Я сравнил два различных способа для маскировки:
1) Маскируется какой то процент выходного состояния по размерности количества семплов в контексте.
2) Маскируется фиксированное количество токенов из вызодного состояния.

Разница между этими подходами заклчается в том, что модель обучается с применением scheduled training'a. То есть количество семплов в контексте меняется во время обучения. Таким образом, количество токенов в первом случае будет также постепенно увеличиваться, в то время как во втором случае оно фиксированною 

Для первого случая я получил следубщие результаты:
![mask state dynamic](/pics/mask_state_dymanic.svg)

Для второго случая я получил следующие результаты:
![mask state static](/pics/mask_state_static.svg)

**Анализ результатов:**

**Для первого случая:**

Как видно из граффиков, при маскировке 10% и 20% выходного состояния, метод продолжает сходиться, но средняя ошибка значительно хуже, чем во варианте без маскировки. Если рассматривать случай с маскированием 40% и 60% выходного состояния, то видно, что метод расходится. 

Из этого можно сделать вывод, что постепенно динамически увеличивать количество токенов, которые будут включать в себя только информацию из входных данных - плохая идея. И даже если модель внутри и хранит информацию об инпуте, то она должна сохраняться в фиксированном количестве токенов. А если постепенно увеличивать количество токенов, которые включают в себя только входные данные, то модель просто не успевает сориентироваться и, соответственно, показывает результаты сильно хуже.

**Для второго случая:**

Чтобы проверить теорию о том, что проблема превого случая именно в том, что количество зануляемых токенов изменяется в процессе обучения, я провел дополнительный эксперимент, где маска над выходным представлением фиксирована. Как видно из результатов, эта теория оправдалась. Все методы начали сходиться сильно лучше, и в конечном счете пришли к результатам почти идентичным обучению обычного Looped Transformer'a. (максимальная разница в MSE в конце обучения - 0.3). 

Для сравнения, вот результаты с динамической маскировкой 10% токенов (в начале обучения 0 токенов, в конце только 4) и статической маскировкой 6 токенов:
![dyn_vs_stat](pics/p_d=0.1VSp_s=6.svg)
Как видно из граффика, методы сходятся к почти идентичному значению, при том, что метод со вариант со статической маскировкой теряет значительно больше информации во время обучения.

Финальный вывод, который можно сделать из данного экспиремента, что изначальная гипотеза подтвердилась, и модель действительно может обучиться на то, чтобы хранить информацию о входных значениях в первых нескольких токенах скрытого представления.

### 3.2 SSM Block testing

**Описание эксперимента:**

Для этого эксперимента Я изменил архитектуру модели, и вместо блока трансформера использовал SSM блок, в частности [Mamba](https://arxiv.org/abs/2312.00752). Похожие работы уже исследовали возможности этой модели в рамках in-context learning'a. В частности, [в этой работе](https://arxiv.org/abs/2312.00752) рассматривались возможности Mamba на идентичной задаче линейной регрессии. В результате исследованя, авторы пришли к выводу, что эта модель показывает результаты на уровне трансформера. Исходя из этого имеет смысл провести эксперименты с этой моделью в рамках моей задачи

**Гипотеза:**

Looped архитектура Mamba будет показывает результат на уровне трансформера на поставленной задаче

**Процесс проведения:**

В качестве реализации архитектуры модели я взял код и репозитория [mamba-minimal](https://github.com/johnma2006/mamba-minimal/tree/master). В нем вся минимальная логика реализована в рамках одного файла, что является аналогом [nanoGPT](https://github.com/karpathy/nanoGPT), использованного в оригинальной статье. 

Для сравнения моделей я подобрал конфигурацию модели, максимально приближенную к конфигурации блока трансформера:

```yaml
model:
    family: mamba_loop
    n_embd: 256
    n_layer: 1
    n_dims: 20
    d_conv: 4
    d_state: 16
    expand: 3

model:
    family: gpt2_loop
    n_embd: 256
    n_layer: 1
    n_head: 8
    n_dims: 20
    n_positions: 101
```
Единственный параметр, который я настраивал для мамбы - expand. Он отвечает за то, во сколько раз размерность внутреннего вектора состояний будет больше размерности ембеддингов. Так как в GPTnano размерность скрытого состояния - 768, то я подогнал expand под соответствующую размерность представлений в мамбе.

Логика для построение Looped версии модели принципиально ничем не отличается от Looped Transformer'a. Если сравнивать результаты обучения обычной Mamba и ее Looped версии, то можно наблюдать следующую картину:
![looped_vs_uplooped_mamba](pics/mamba_loop_vs_unloop.svg)
Как видно из граффика, Looped версия показала значительно лучшие результаты при равнов количестве параметров и сошлась к значению ошибки в ~2 раза меньше, чем у базовой версии. Из этого можно сделать вывод, что схожая идея с итеративным in-context learning'ом применима и для данного типа модели, а не только для трансформеров.

Если сравнивать процесс обучения LoopedMamba и LoopedTransformer, то получим следующий результат:
![loop_mamba_tf](pics/mamba_vs_transformer_looped.svg)
Как видно из графика, то разница lossa заключается в следующих пунктах:
* В началае обучения Mamba сходится быстрее
* В определенный момент Transformer сходится к решению лучше (хоть и не на много)

Чтобы выявить причину второго пункта, исследуем граффики сходимости отдельно для каждого семпла:
Для большинства примеров, разница в ошибке несущественна, максимальная разница лежит за весь период обучения лежит в районе 0.2-0.3, а к концу обучения вообще сходится к ~0.03.
![alt text](pics/point_diffs_small.png)

Однако, на некоторых примерах даже к концу обучения наблюдяется существенная разница в лоссе:
![alt text](pics/point_diffs_big.png)

Если вспомнить изначальный сетап задачи, то можно составить предположение о том, в чем причина этой разницы. Как было описано ранее, для обучения используется 


## 4. Общий анализ
- Сравнение результатов всех экспериментов
- Объяснение наблюдаемого поведения
- Выводы о эффективности различных подходов

## 5. Заключение
- Краткое резюме основных находок
- Обсуждение потенциальных улучшений и будущих направлений исследования

## 6. Приложения
- Инструкции по запуску кода
- Ссылки на используемые ресурсы и библиотеки
- Дополнительные графики или данные (если есть)